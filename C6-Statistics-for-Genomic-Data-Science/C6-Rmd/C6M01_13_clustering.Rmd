---
title: Clustering
original author: Jeff Leek
reorganization: Yuki Nakayama
output:
  rmarkdown::html_document: 
    toc: true
  vignette: >  
    %\VignetteIndexEntry{Clustering for genomics}
    %\VignetteEngine{knitr::rmarkdown}
    \usepackage[utf8]{inputenc}
---

```{r front, child="C:\\path\\to\\your\\front.Rmd"}
```


## Genomic Data Science Specialization

Johns Hopkins University

Course 6 : Statistics for Genomic Data Science

Module 1

Clustering in R (9:09)

## Dependencies

This document depends on the following packages:

```{r load_hidden, echo=FALSE, results="hide", warning=FALSE}
suppressPackageStartupMessages({
  library(devtools)
  library(Biobase)
  library(dendextend)
})
```

Dendextend package in order to enhance the visual appeal of dendograms.

```{r load}
  library(devtools)
  library(Biobase)
  library(dendextend)
```

To install these packages you can use the code (or if you are compiling the document, remove the `eval=FALSE` from the chunk.)

```{r install_packages, eval=FALSE}
install.packages(c("devtools","dendextend"))
source("http://www.bioconductor.org/biocLite.R")
biocLite(c("Biobase"))
```


## General principles

* How do we define close?
* How do we group things?
* How do we visualize the grouping? 
* How do we interpret the grouping? 


## Load some data

We will use this expression set to look at how we use plots and tables to check for different characteristics

```{r}
con =url("http://bowtie-bio.sourceforge.net/recount/ExpressionSets/bodymap_eset.RData")
load(file=con)
close(con)
bm = bodymap.eset
pdata=pData(bm)
edata=as.data.frame(exprs(bm))
fdata = fData(bm)
ls()
```

## Calculate distances between samples

* Most important step
  * Garbage in -> garbage out
* Distance or similarity
  * Continuous - euclidean distance
  * Continous - correlation similarity
  * Binary - manhattan distance
* Pick a distance/similarity that makes sense for your problem

First we log transform and remove lowly expressed genes, then calculate Euclidean distance

```{r}
edata = edata[rowMeans(edata) > 5000,]
dim(edata)
```

It is necessary to require that the mean count exceed 5000. Consequently, applying this process yields a data set comprising approximately 1,100 or 1,200 genes. The subsequent step is to perform a log2 transformation.

```{r}
edata = log2(edata + 1)

# By default calculates the distance between rows
dist1 = dist(t(edata))
dist1
```
The distances between the samples can be observed, as illustrated by the diagonal line, which represents the distance between a sample and itself. It is evident that the distance in question is minimal. It is evident that some samples are in closer proximity to one another, as illustrated by the white clusters. Conversely, other samples exhibit greater distances between them, as exemplified by the 19th, 18th, and 19th samples, which are relatively distant from one another. This is indicated by the blue distances. 

```{r}
## Look at distance matrix
colramp = colorRampPalette(c(3,"white",2))(9)
heatmap(as.matrix(dist1),col=colramp,Colv=NA,Rowv=NA)
```

Subsequently, clustering can be performed on this data set. The H-clust function is employed for hierarchical clustering.

## Now cluster the samples

Here we use the distance we previously calculated to perform a hierarchical clustering and plot the dendrogram:
```{r}
hclust1 = hclust(dist1)
plot(hclust1)
```

We can also force all of the leaves to terminate at the same spot

```{r}
plot(hclust1,hang=-1)
```

We can also color the dendrogram either into a fixed number of groups

```{r}
dend = as.dendrogram(hclust1)
dend = color_labels(hclust1,4,col=1:4)
plot(dend)
```

The number of clusters can be specified. To view three clusters, the number of clusters and colours may be modified and a plot generated.

```{r}
dend = color_labels(hclust1,3,col=1:3)
plot(dend)
```

### Or you can color them directly 

The initial ten samples come from one case, the subsequent nine from another. Labels can be defined directly, with the first ten samples labelled 'one' and the subsequent nine 'two'. Samples can then be plotted according to their labels, as illustrated. This allows for the clear visual separation of the two groups.

```{r}
labels_colors(dend) = c(rep(1,10),rep(2,9))
plot(dend)
```

This can get very fancy and we won't cover all the options here but for further ideas check out the [dendextend package](http://htmlpreview.github.io/?https://github.com/talgalili/dendextend/blob/master/inst/ignored/Introduction%20to%20dendextend.html).

## K-means clustering

Now we can perform k-means clustering. By default, the rows are clustered. You can either input the cluster means (often unknown) or the number of clusters (here I input 3 clusters)

```{r}
kmeans1 = kmeans(edata,centers=3)
names(kmeans1)
```

### Now we can look at the cluster centers

The data set comprises three distinct cluster centres. The middle samples exhibit a considerably higher value than the other samples, indicating a prominent cluster comprising these points. Three distinct cluster centres were identified and each gene was assigned to the most closely associated centre. 

```{r}
matplot(t(kmeans1$centers),col=1:3,type="l",lwd=3)
```

### We can also look at how many belong to each cluster

Examine the number of genes in each cluster and determine which cluster each gene belongs to. 396 genes are assigned to the third cluster, 739 to the second, and the rest to the other clusters.  

```{r}
table(kmeans1$cluster)
kmeans1$cluster[1:10]
```

### And cluster the data together and plot

A clustered version will be created using the k-mean clustering method. Genes with similar characteristics have been grouped together.

```{r}
heatmap(as.matrix(edata)[order(kmeans1$cluster),],col=colramp,Colv=NA,Rowv=NA)
```

Note that this is a random start so you get a different clustering if you run it again you get a different answer

```{r}
kmeans2 = kmeans(edata,centers=3)
table(kmeans1$cluster,kmeans2$cluster)
```

The process is not deterministic. The process starts at random. The k-means clusters were recalculated and a table of the resulting cluster membership was created. It became evident that in this instance, clusters two and one were classified as the other in the other sample, and vice versa. The number of iterations and starts can be modified according to user preferences. The k-means documentation provides detailed information on these parameters. The number of random variables, clusters, and centres can also be modified. Increasing the number sufficiently will produce similar results. 

## Session information

Here is the session information 

```{r session_info}
devtools::session_info()
```

It is also useful to compile the time the document was processed. This document was processed on: `r Sys.Date()`.

